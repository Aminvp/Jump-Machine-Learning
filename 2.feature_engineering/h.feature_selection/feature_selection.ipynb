{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "انتخاب ویژگی\n",
    "</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "مجموعه‌داده\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مثال از مجموعه‌داده‌ی MNIST استفاده می‌کنیم که می‌توان آن را مستقیماً به کمک کتابخانه‌ی <code>scikit-learn</code> بارگیری کرد.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/Desktop/Jump Machine Learning/qenv/lib/python3.9/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/home/amin/Desktop/Jump Machine Learning/qenv/lib/python3.9/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# loading the mnist dataset\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "آستانه‌ی واریانس (Variance Threshold)\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "برای این کار می‌توانیم از تابع <code>sklearn.feature_selection.VarianceThreshold</code> استفاده کنیم. این تابع ویژگی‌هایی را که واریانس مجموعه آموزشی‌شان کمتر از مقدار تنظیم‌شده برای <code>threshold</code> باشد حذف می‌کند. به‌صورت پیش‌فرض این آرگومان معادل <code>0.0</code> تنظیم می‌شود که بدین معنی‌ست ویژگی‌های با واریانس صفر (مقدار ویژگی در تمام نمونه‌ها مشابه و ثابت است) حذف شوند.\n",
    "در کد زیر سعی داریم از تکنیک <code>varianceThreshold</code> استفاده کرده و ویژگی‌هایی که واریانس‌شان کمتر از <code>0.01</code> است را حذف کنیم. تنها توجه داشته باشید که پیش از این کار باید ویژگی‌ها را نیز هم‌مقیاس کنیم تا تصمیم‌گیری‌مان عادلانه باشد.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before selection: 784\n",
      "Number of features after selection: 719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "\n",
    "# feature selection\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selected = selector.fit_transform(scaled)\n",
    "\n",
    "print('Number of features before selection:', X.shape[1])\n",
    "print('Number of features after selection:', selected.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "تحلیل تک‌متغیره (Univariate analysis)\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    " تابع <code>SelectKBest</code>، عدد <code>k</code> را می‌گیرد و با توجه به آزمون آماری تعیین‌شده، <code>k</code> تا از بهترین ویژگی‌ها از نظر ارتباط با ویژگی هدف را برمی‌گرداند.\n",
    " برای آزمون‌های آماری نیز می‌توانیم از توابع موجود در کتابخانه‌ی  <code>scikit-learn</code> همچون آزمون <code>chi2</code> کمک بگیریم.\n",
    " در مثال زیر قصد داریم براساس آزمون آماری <code>chi2</code>، تنها <code>32</code>تا از مرتبط‌ترین ویژگی‌ها با متغیر هدف را پیدا کرده و نگه داریم.\n",
    " توجه شود از آنجا که آزمون آماری <code>chi2</code> تنها مقادیر غیرمنفی را می‌پذیرد این‌بار از تغییر مقیاس با روش <code>MinMaxScaler</code> استفاده کرده‌ایم. البته در این مجموعه‌داده‌ی خاص مقادیر از ابتدا غیرمنفی هستند اما نیاز است که به این نکته توجه داشته باشید.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before selection: 784\n",
      "Number of features after selection: 32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "\n",
    "# feature selection\n",
    "selector = SelectKBest(chi2, k=32)\n",
    "selected = selector.fit_transform(scaled, y)\n",
    "\n",
    "print('Number of features before selection:', X.shape[1])\n",
    "print('Number of features after selection:', selected.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "هم‌بستگی جفت جفت ویژگی‌ها (Feature pairwise correlation)\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    این تکنیک مستقیماً در <code>scikit-learn</code> پیاده‌سازی نشده، اما خودمان می‌توانیم در چند خط کد پیاده‌اش کنیم.\n",
    "    تابع <code dir=ltr>corr()</code> می‌تواند به‌راحتی هم‌بستگی جفت ویژگی‌ها را برای‌مان محاسبه کند. البته از آنجا که به دنبال میزان هم‌بستگی هستیم و علامت عدد برای‌مان اهمیت ندارد از <code dir=ltr>abs()</code> استفاده می‌کنیم تا مقادیر منفی نیز به مثبت تبدیل شوند.\n",
    "    توجه داشته باشید که ماتریس هم‌بستگی یک ماتریس متقارن است و عناصر تکراری دارد، بنابراین می‌توانیم تنها بخش بالا مثلثی آن را نگه داریم. پس از این کار کافیست هر کدام از ستون‌ها (ویژگی‌ها) را بررسی کنیم و اگر مقدار هم‌بستگی آن با حداقل یکی از ویژگی‌های دیگر بیشتر از حد آستانه بود، می‌توانیم آن ویژگی را حذف کنیم.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature(s) to drop: pixel16, pixel87, pixel225, pixel281, pixel421, pixel617, pixel644, pixel646, pixel727\n",
      "Number of features before selection: 784\n",
      "Number of features after selection: 775\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# computing the absolute values of correlation\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# keeping only upper part of correlation matrix\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# finding the highly correlated features\n",
    "THRESHOLD = 0.90\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > THRESHOLD)]\n",
    "print('Feature(s) to drop:', ', '.join(to_drop))\n",
    "\n",
    "# dropping the highly correlated features\n",
    "selected = X.drop(to_drop, axis=1)\n",
    "\n",
    "print('Number of features before selection:', X.shape[1])\n",
    "print('Number of features after selection:', selected.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "qenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
